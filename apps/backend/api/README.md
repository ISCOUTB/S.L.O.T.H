# ETL Design API Service

A comprehensive FastAPI-based REST API that serves as the main interface for the ETL Design system. This service handles user authentication, file validation requests, schema management, and communication with backend processing services.

## ğŸš€ Overview

The API Service is the primary entry point for users and external applications to interact with the ETL Design system. It provides a complete REST API with authentication, user management, file upload capabilities, and real-time status tracking for validation and schema operations.

## âœ¨ Key Features

- **ğŸ” Authentication & Authorization**: JWT-based authentication with role-based access control
- **ğŸ‘¥ User Management**: Complete CRUD operations for user accounts with admin controls
- **ğŸ“‹ Schema Management**: Upload, update, and remove JSON schemas with versioning support
- **ğŸ“„ File Validation**: Upload spreadsheet files (CSV, XLSX, XLS) for validation against schemas
- **ğŸ—ï¸ Excel Parsing Workflow**: Orchestrates complete ETL process from validation to SQL generation
- **ğŸ’¾ Intelligent Caching**: Redis-based response caching for improved performance
- **ğŸ”„ Asynchronous Processing**: RabbitMQ integration for background task processing
- **ğŸŒ REST API**: Comprehensive endpoints with automatic OpenAPI documentation
- **ğŸ¥ Health Monitoring**: System health checks and status endpoints
- **ğŸ”’ Type Safety**: Full Pydantic model validation and type checking

## Architecture

The API Service acts as the communication layer between users and the processing backend:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client Apps   â”‚â”€â”€â”€â”€â–¶â”‚   API Service   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   RabbitMQ      â”‚
â”‚   (Web/Mobile)  â”‚     â”‚   (FastAPI)     â”‚             â”‚   (Publisher)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â–¼
                        â–¼                   â–¼                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  Typechecking   â”‚
                â”‚  Database Svc   â”‚ â”‚   PostgreSQL    â”‚  â”‚   Workers       â”‚
                â”‚  (gRPC Proxy)   â”‚ â”‚ (Direct SQLAlch)â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                    â–²
                        â–¼                    â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
                â”‚ Redis + MongoDB â”‚          â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                             â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚ User Management,  â”‚
                                   â”‚ Authentication,   â”‚
                                   â”‚ Application Data  â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Separation

- **Cache & Schemas**: API â†” Database Service (gRPC) â†” Redis/MongoDB  
- **Users & Auth**: API â†” PostgreSQL (Direct SQLAlchemy)

## ğŸ—ï¸ ETL Workflow Orchestration

The API Service orchestrates the complete ETL process from file validation to SQL generation. This workflow integrates typechecking validation with excel parsing services to provide end-to-end data transformation.

### Complete ETL Pipeline

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   File Upload   â”‚â”€â”€â”€â–¶â”‚   Validation    â”‚â”€â”€â”€â–¶â”‚ Excel Parsing   â”‚
â”‚   (API Service) â”‚    â”‚ (Typechecking)  â”‚    â”‚   (Parsers)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Schema Check   â”‚    â”‚ Data Validation â”‚    â”‚ SQL Generation  â”‚
â”‚  (Database Svc) â”‚    â”‚  (Polars/JSON)  â”‚    â”‚ (DDL + INSERT)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow Stages

1. **ğŸ“„ File Upload & Schema Validation**
   - API receives Excel/CSV file upload
   - Validates file format and basic structure
   - Checks if corresponding JSON schema exists

2. **ğŸ” Data Validation (via Typechecking)**
   - API publishes validation message to RabbitMQ
   - Typechecking workers validate data against schema
   - Results include detailed validation reports

3. **ğŸ—ï¸ Excel Parsing Pipeline** *(Planned Integration)*
   - **Formula Extraction**: Parse Excel formulas and dependencies
   - **DDL Generation**: Create table definitions from validated data
   - **SQL Building**: Generate INSERT statements from clean data
   - **Result Consolidation**: Return complete SQL package

### Future ETL Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/etl/process/{import_name}` | Complete ETL: validation + parsing + SQL |
| `GET` | `/api/v1/etl/status/{task_id}` | Track ETL pipeline progress |
| `GET` | `/api/v1/etl/results/{task_id}` | Download generated SQL files |
| `POST` | `/api/v1/etl/validate-and-parse` | Two-stage validation then parsing |

### Integration Points

- **Typechecking Service**: Data validation and schema compliance
- **Excel Reader**: Formula extraction and data processing
- **Formula Parser**: Complex formula dependency analysis  
- **DDL Generator**: Database schema generation
- **SQL Builder**: INSERT statement generation

**Note**: The excel parsing integration is currently in development. The API Service will serve as the orchestration layer that coordinates the workflow between validation (typechecking) and parsing (excel-reader) services.

## ğŸ”Œ API Endpoints

The API provides comprehensive REST endpoints with automatic OpenAPI documentation available at `/docs`.

### ğŸ” Authentication

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/login/access-token` | Login and get JWT access token |
| `GET` | `/api/v1/login/test-token` | Test token validity |

### ğŸ‘¥ User Management

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/v1/users/info` | Get current user information |
| `GET` | `/api/v1/users/search/{username}` | Get specific user details |
| `GET` | `/api/v1/users/search` | List all users (paginated) |
| `POST` | `/api/v1/users/create` | Create new user |
| `PATCH` | `/api/v1/users/update/{username}` | Update user information |
| `DELETE` | `/api/v1/users/delete/{username}` | Delete user |

### ğŸ·ï¸ Schema Management

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/schemas/upload/{import_name}` | Upload JSON schema with versioning |
| `GET` | `/api/v1/schemas/status` | Get schema upload status and metadata |
| `DELETE` | `/api/v1/schemas/remove/{import_name}` | Remove schema with rollback support |

### ğŸ“„ File Validation

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/validation/upload/{import_name}` | Upload and validate spreadsheet files |
| `GET` | `/api/v1/validation/status` | Check validation task status and progress |

### ğŸ’¾ Cache Management

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/v1/cache` | Get cache statistics and stored keys |
| `DELETE` | `/api/v1/cache/clear` | Clear all cached data |

### ğŸ¥ Health & Monitoring

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Basic health check with service status |
| `GET` | `/health/detailed` | Detailed health info including dependencies |
| `GET` | `/metrics` | Application metrics for monitoring systems |

## ğŸ’¡ Usage Examples

### Authentication Flow

```bash
# Login to get access token
curl -X POST "http://localhost:8000/api/v1/login/access-token" \
     -H "Content-Type: application/x-www-form-urlencoded" \
     -d "username=admin&password=admin&rol=admin"

# Use token for authenticated requests
export TOKEN="<your_access_token>"
curl -H "Authorization: Bearer $TOKEN" \
     "http://localhost:8000/api/v1/users/info"
```

### Schema Management

```bash
# Upload a JSON schema
curl -X POST "http://localhost:8000/api/v1/schemas/upload/user_data" \
     -H "Authorization: Bearer $TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "type": "object",
       "properties": {
         "name": {"type": "string"},
         "email": {"type": "string", "format": "email"},
         "age": {"type": "integer", "minimum": 0}
       },
       "required": ["name", "email"]
     }'

# Check schema status
curl -H "Authorization: Bearer $TOKEN" \
     "http://localhost:8000/api/v1/schemas/status?import_name=user_data"
```

### File Validation

```bash
# Upload and validate a CSV file
curl -X POST "http://localhost:8000/api/v1/validation/upload/user_data" \
     -H "Authorization: Bearer $TOKEN" \
     -F "spreadsheet_file=@users.csv"

# Check validation status
curl -H "Authorization: Bearer $TOKEN" \
     "http://localhost:8000/api/v1/validation/status?import_name=user_data"
```

### User Management

```bash
# Create a new user
curl -X POST "http://localhost:8000/api/v1/users/create" \
     -H "Authorization: Bearer $TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "username": "newuser",
       "email": "newuser@example.com",
       "full_name": "New User",
       "password": "securepassword"
     }'

# Get user information
curl -H "Authorization: Bearer $TOKEN" \
     "http://localhost:8000/api/v1/users/search/newuser"
```

## âš™ï¸ Configuration

The service uses environment variables for configuration. Create a `.env` file based on `.env.example`.

### API Configuration

```bash
# Server Settings
SERVER_HOST="localhost"
SERVER_PORT=8000
SERVER_DEBUG=false
API_V1_STR="/api/v1"
CORS_ORIGINS="http://localhost,http://localhost:3000,http://localhost:8000"

# Security
SECRET_KEY="your_secret_key_here"
FIRST_SUPERUSER="admin"
FIRST_SUPERUSER_PASSWORD="admin_password"

# Health Monitoring
HEALTH_CHECK_ENABLED=true
HEALTH_ENDPOINTS_INCLUDE_DETAILED=true
```

### Database Configuration

```bash
# PostgreSQL (User Management - Direct Connection)
POSTGRES_HOST="localhost"
POSTGRES_PORT=5432
POSTGRES_USER="admin"
POSTGRES_PASSWORD="admin"
POSTGRES_DB="typechecking_db"

# Database Service (gRPC - Redis/MongoDB Operations)
DATABASE_CONNECTION_HOST="localhost"
DATABASE_CONNECTION_PORT=50050
```

### RabbitMQ Configuration

```bash
# RabbitMQ Publishing
RABBITMQ_HOST="localhost"
RABBITMQ_PORT=5672
RABBITMQ_USER="admin"
RABBITMQ_PASSWORD="admin"
RABBITMQ_VHOST="/"

# Worker Configuration
MAX_WORKERS=4
WORKER_CONCURRENCY=4
WORKER_PREFETCH_COUNT=1

# Exchange and Queues
RABBITMQ_EXCHANGE="typechecking.exchange"
RABBITMQ_EXCHANGE_TYPE="topic"
RABBITMQ_QUEUE_SCHEMAS="typechecking.schemas.queue"
RABBITMQ_QUEUE_VALIDATIONS="typechecking.validations.queue"

# Publishing Routing Keys
RABBITMQ_PUBLISHERS_ROUTING_KEY_SCHEMAS="schemas.update"
RABBITMQ_PUBLISHERS_ROUTING_KEY_VALIDATIONS="validation.request"
```

## ğŸ› ï¸ Development

### Prerequisites

- Python 3.12+
- PostgreSQL 17+
- RabbitMQ 4.0+
- Database Service running

### Installation

```bash
# Install dependencies
uv sync

# Run database migrations
uv run alembic upgrade head

# Create initial data
uv run python -m src.initial_data
```

Or just running the script [prestart.sh](./scripts/prestart.sh).

### Running the Service

```bash
# Development mode
uv run uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload

# Production mode
uv run uvicorn src.main:app --host 0.0.0.0 --port 8000
```

### Testing

```bash
# Run tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html
```

## ğŸ—ï¸ Project Structure

```text
api/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/               # FastAPI routes and dependencies
â”‚   â”‚   â”œâ”€â”€ routes/        # API endpoint definitions
â”‚   â”‚   â”œâ”€â”€ deps.py        # Dependency injection
â”‚   â”‚   â”œâ”€â”€ main.py        # Router configuration
â”‚   â”‚   â””â”€â”€ utils.py       # API utilities
â”‚   â”œâ”€â”€ controllers/       # Business logic layer
â”‚   â”œâ”€â”€ core/             # Configuration and database
â”‚   â”œâ”€â”€ messaging/        # RabbitMQ publisher
â”‚   â”œâ”€â”€ models/           # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas/          # Pydantic models
â”‚   â”œâ”€â”€ utils/            # Utility functions
â”‚   â””â”€â”€ main.py           # Application entry point
â”œâ”€â”€ scripts/              # Deployment scripts
â”œâ”€â”€ logs/                 # Application logs
â””â”€â”€ alembic/             # Database migrations
```

## ğŸ”„ Communication Flow

### Request Processing

1. **Client Request**: HTTP request to FastAPI endpoint
2. **Authentication**: JWT token validation and user authorization
3. **Business Logic**: Controllers handle business rules and validation
4. **Database Operations**: Via Database Service (gRPC) or direct SQL
5. **Message Publishing**: Async tasks sent to RabbitMQ
6. **Response**: Immediate response with task ID for tracking

### Background Processing

1. **Message Publishing**: API publishes to RabbitMQ queues
2. **Worker Consumption**: Typechecking service consumes messages
3. **Status Updates**: Workers update task status via Database Service
4. **Result Retrieval**: Clients poll status endpoints for completion

### ETL Pipeline Processing *(Future)*

1. **File Upload**: Client uploads Excel/CSV file with schema reference
2. **Validation Stage**: API coordinates validation via Typechecking service
3. **Parsing Stage**: API coordinates Excel parsing via Parser services
4. **SQL Generation**: DDL Generator and SQL Builder create database scripts
5. **Result Delivery**: Complete SQL package returned to client

## ğŸ¤ Integration Points

- **Database Service**: gRPC client for Redis/MongoDB operations
- **Typechecking Service**: RabbitMQ message publishing for async processing
- **Excel Reader Service**: *(Planned)* REST/gRPC communication for file parsing
- **Formula Parser Service**: *(Planned)* Complex formula analysis integration
- **DDL Generator Service**: *(Planned)* Database schema generation
- **SQL Builder Service**: *(Planned)* INSERT statement generation
- **Frontend Applications**: REST API endpoints for UI integration
- **External Systems**: Authentication and user management for third-party apps

## Related Documentation

- [Typechecking Service](../typechecking/): Background processing workers
- [Database Service](../../connections/database/): Centralized database operations
- [Protocol Definitions](../../../packages/proto/): Shared interface specifications
